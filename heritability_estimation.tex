\chapter{Heritability Estimation}

% Need to stress that we are only calculating the narrow sense heritability
	\section{Introduction}
	The development of \glng{ldsc} has brought great prospect in estimating the heritability of complex disease for one can now estimate the heritability of a trait without requiring the rare genotype. 
	However, as noted by the author of \gls{ldsc}, when the number of causal variants were small, or when working on targeted genotype array, \gls{ldsc} tends to have a larger standard error or might produce funky results\citep{Bulik-Sullivan2015}. 
	Thus, there might be room for improvement of the method such that the performance of heritability estimation should be robust to all possible condition (e.g. number of causal SNPs, type of genotyping array).
	
	In this chapter, we would introduce our method of heritability estimation using test static called \gls{shrek}. We will compare the performance of \gls{shrek} and \gls{ldsc} using simulation of different genetic architectures. 
	
	
	\section{Methodology}	
		The overall aims of this study is to develop a robust algorithm for the estimation of the narrow sense heritability using only the summary statistic from a \gls{GWAS} study. 
		The work in this chapter were done in collaboration with my colleagues who have kindly provide their support and knowledges to make this piece of work possible.
		Dr Johnny Kwan, Dr Miaxin Li and Professor Sham have helped to laid the framework of this study. 
		Dr Timothy Mak has derived the mathematical proof for our heritability estimation method. 
		Miss Yiming Li, Dr Johnny Kwan, Dr Miaxin Li, Dr Timothy Mak and Professor Sham have helped with the derivation of the standard error of the heritability estimation. 
		Dr Henry Leung has provided critical suggestions on the implementation of the algorithm.
		
		\subsection{Heritability Estimation}
			The narrow-sense heritability is defined as 
			$$
				h^2 = \frac{\mathrm{Var}(X)}{\mathrm{Var}(Y)}
			$$
			where $\mathrm{Var}(X)$ is the variance of the genotype and $\mathrm{Var}r(Y)$ is the variance of the phenotype.
			In a \gls{GWAS}, regression were performed between the \glspl{SNP} and the phenotypes, giving
			\begin{equation}
				Y=\beta X+\epsilon
				\label{eq:standardRegress}
			\end{equation}
			where $Y$ and $X$ are the standardized phenotype and genotype respectively. 
			$\epsilon$ is then the error term, accounting for the non-genetic elements contributing to the phenotype (e.g. Environment factors).
			Based on \cref{eq:standardRegress}, one can then have
			\begin{align}
				\mathrm{Var}(Y) = \mathrm{Var}(\beta X)+ \mathrm{Var}(\epsilon) \nonumber\\
				\mathrm{Var}(Y) = \beta^\mathrm{Var}(X) \nonumber\\
				\beta^2\frac{\mathrm{Var}(X)}{\mathrm{Var}(Y)}= 1
				\label{eq:betaHeri}
			\end{align}
			$\beta^2$ is then considered as the portion of phenotype variance explained by the variance of genotype, which can also be considered as the narrow-sense heritability of the phenotype.
					
			A challenge in calculating the heritability from \gls{GWAS} data is that usually only the test-statistic or p-value were provided and one will not be able to directly calculate the heritability based on \cref{eq:betaHeri}. In order to estimation the heritability of a trait from the \gls{GWAS} test-statistic, we first observed that when both $X$ and $Y$ are standardized, $\beta^2$ will be equal to the coefficient of determination ($r^2$). Then, based on properties of the Pearson product-moment correlation coefficient:
			\begin{equation}
				r = \frac{t}{\sqrt{n-2+t^2}}
				\label{eq:pearsonProduct}
			\end{equation}
			where $t$ follows the student-t distribution and $n$ is the number of samples.
			One can then obtain the $r^2$ by taking the square of \cref{eq:pearsonProduct}
			\begin{equation}
				r^2 = \frac{t^2}{n-2+t^2}
				\label{eq:oriRSquared}
			\end{equation}
			It is observed that $t^2$ will follow the F-distribution and when $n$ is big, $t^2$ will converge into $\chi^2$ distribution.
			
			When the effect size is small and $n$ is big, $r^2$ will be approximately $\chi^2$ distributed with mean $\sim 1$. 
			We can then approximate \cref{eq:oriRSquared} as
			\begin{equation}
				r^2= \frac{\chi^2}{n}
				\label{eq:approxChi}
			\end{equation}
			and define the \emph{observed} effect size of each \gls{SNP} to be
			\begin{equation}
			f=\frac{\chi^2-1}{n}
			\label{eq:observedEffect}
			\end{equation}
			
			When there are \gls{LD} between each individual \glspl{SNP}, the situation will become more complicated as each \glspl{SNP}' observed effect will contains effect coming from other \glspl{SNP} in \gls{LD} with it. 
			\begin{equation}
			f_{observed} = f_{true}+f_{LD}
			\label{eq:conceptF}
			\end{equation}
			
			To account for the \gls{LD} structure, we first assume our phenotype $\boldsymbol{Y}$ and genotype $\boldsymbol{X}=(X_1,X_2,\dots,X_m)^t$ are standardized and that
			\begin{align*}
				\boldsymbol{Y}\sim f(0,1) \\
				\boldsymbol{X}\sim f(0,\boldsymbol{R})
			\end{align*}
			Where $\boldsymbol{R}$ is the \gls{LD} matrix between \glspl{SNP}.
			
			We can then express \cref{eq:standardRegress} in matrix form:
			\begin{align}
				\boldsymbol{Y}=\boldsymbol{\beta}^t\boldsymbol{X}+\epsilon
				\label{eq:matrixRegress}
			\end{align}
			Definition of heritability will then become
			\begin{align}
				Heritability& = \frac{\mathrm{Var}(\boldsymbol{\beta}^t\boldsymbol{X})}{\mathrm{Var}(\boldsymbol{Y})} \nonumber\\
				&=\mathrm{Var}(\boldsymbol{\beta}^t\boldsymbol{X})
			\end{align}
			If we then assume now that $\boldsymbol{\beta} = (\beta_1, \beta_2,\dots,\beta_m)^t$ has distribution
			\begin{align*}
				\boldsymbol{\beta}&\sim f(0,\boldmath{H})\\
				\boldsymbol{H}&=diag(\boldsymbol{h})\\
				\boldsymbol{h}&=(h_1^2,h_2^2,\dots,h_m^2)^t
			\end{align*}
			where $\boldsymbol{H}$ is the variance of the true effect. 
			It is shown that heritability can be expressed as %The later part was gone because that will contains E(\beta) which = 0
			\begin{align}
			\mathrm{Var}(\boldsymbol{\beta}^t\boldsymbol{X}) &= \mathrm{E}_X\mathrm{Var}_{\beta|X}(\boldsymbol{X}^t\boldsymbol{\beta})+\mathrm{Var}_X\mathrm{E}_{(\beta|X)}(\boldsymbol{\beta}^2\boldsymbol{X}) \nonumber\\
			&=\mathrm{E}_X(\boldsymbol{X}^t\boldsymbol{\beta\beta}^T\boldsymbol{X}) \nonumber\\ 
			&= \mathrm{E}_X(\boldsymbol{X}^t\boldsymbol{HX}) \nonumber\\
			&= \mathrm{E}(\boldsymbol{X})^t\boldsymbol{H}\mathrm{E}(\boldsymbol{X})+\mathrm{Tr}(\mathrm{Var}(\boldsymbol{X}\boldsymbol{H})) \nonumber\\
			&=\mathrm{Tr}(\mathrm{Var}(\boldsymbol{X}\boldsymbol{H})) \nonumber\\
			&=\sum_ih_i^2
			\label{eq:proveHerit}
			\end{align}
			
			Now if we consider the covariance between \gls{SNP} i ($X_i$) and $Y$, we have
			\begin{align}
			 \mathrm{Cov}(\boldsymbol{X}_i,\boldsymbol{Y}) &= \mathrm{Cov}(\boldsymbol{X}_i,\boldsymbol{\beta}^t\boldsymbol{X}+\epsilon) \nonumber\\
			 &=\mathrm{Cov}(\boldsymbol{X}_i,\boldsymbol{\beta}^t\boldsymbol{X}) \nonumber\\
			 &=\sum_j{\mathrm{Cov}(\boldsymbol{X}_i,\boldsymbol{X}_j)\boldsymbol{\beta}_j} \nonumber\\
			 &=\boldsymbol{R}_i\boldsymbol{\beta}_j
			 \label{eq:covPhenoTrue}
			\end{align}
			As both $X$ and $Y$ are standardized, the covariance will equal to the correlation and we can define the correlation between \gls{SNP} i and $Y$ as
			\begin{equation}
				\rho_i = \boldsymbol{R}_i\boldsymbol{\beta}_j
				\label{eq:corPhenoTrue}
			\end{equation}
			In reality, the \emph{observed} correlation usually contains error. 
			Therefore we define the \emph{observed} correlation to be
			\begin{equation}
			\hat{\rho_i} = \rho_i+\frac{\epsilon_i}{\sqrt{n}}
			\label{eq:obsPheno}
			\end{equation}
			for some error $\epsilon_i$. 
			The distribution of the correlation coefficient about the true correlation $\rho$ is approximately
			$$
				\hat{\rho_i}\sim f(\rho_i, \frac{(1-\rho^2)^2}{n})
			$$
			By making the assumption that $\rho_i$ is close to 0 for all $i$, we have 
			\begin{align*}
				\mathrm{E}(\epsilon_i|\rho_i)&\sim 0\\
				\mathrm{Var}(\epsilon_i|\rho_i)&\sim 1
			\end{align*}
			We then define our $z$-statistic and $\chi^2$-statistic as
			\begin{align*}
				z_i &= \hat{\rho_i}\sqrt{n} \\
				\chi^2 &= z_i^2\\
				&=\hat{\rho_i}^2n
			\end{align*}
			From \cref{eq:obsPheno} and \cref{eq:corPhenoTrue}, $\chi^2$ can then be expressed as
			\begin{align*}
			\chi^2&=\hat{\rho}^2n\\
			&=n(\boldsymbol{R}_i\boldsymbol{\beta}_j+\frac{\epsilon_i}{\sqrt{n}})^2
			\end{align*}
			The expectation of $\chi^2$ is then
			\begin{align*}
			\mathrm{E}(\chi^2) &= n(\boldsymbol{R}_i\boldsymbol{\beta\beta}^t\boldsymbol{R}_i+2\boldsymbol{R}_i\boldsymbol{\beta}\frac{\epsilon_i}{\sqrt{n}}+\frac{\epsilon_i^2}{n}) \\
			&= n\boldsymbol{R}_i\boldsymbol{H}\boldsymbol{R}_i+1
			\end{align*}
			To derive least square estimates of $h_i^2$, we need to find $\hat{h_i^2}$ which minimizes
			\begin{align*}
				\sum_i(\chi_i^2-\mathrm{E}(\chi_i^2))^2&=\sum_i(\chi_i^2-(n\boldsymbol{R}_i\boldsymbol{H}\boldsymbol{R}_i+1))^2 \\
				&=\sum_i(\chi_i^2-1-n\boldsymbol{R}_i\boldsymbol{H}\boldsymbol{R}_i)^2 
			\end{align*}
			If we define 
			\begin{equation}
			f_i= \frac{\chi_i^2-1}{n}
			\label{eq:defineF}
			\end{equation}
			we got
			\begin{align}
			\sum_i(\chi_i^2-\mathrm{E}(\chi_i^2))^2&=\sum_i(f_i-\boldsymbol{R}_i\boldsymbol{H}\boldsymbol{R}_i)^2 \nonumber\\
			&=\boldsymbol{ff}^t-2\boldsymbol{f}^t\boldsymbol{R_{sq}\hat{h}}+\boldsymbol{\hat{h}}^t\boldsymbol{R_{sq}}^t\boldsymbol{R_{sq}\hat{h}}
			\label{eq:leastSquareH}
			\end{align}
			where $\boldsymbol{R_{sq}} = \boldsymbol{R}\circ\boldsymbol{R}$.
			By differentiating \cref{eq:leastSquareH} w.r.t $\hat{h}$ and set to 0, we get
			\begin{align}
				2\boldsymbol{R_{sq}}^t\boldsymbol{R_{sq}}\boldsymbol{\hat{h^2}}-2\boldsymbol{R_{sq}f}&=0 \nonumber\\
				\boldsymbol{R_{sq}}\boldsymbol{\hat{h^2}} &=\boldsymbol{f}
				\label{eq:shrekEq}
			\end{align}
			And the heritability is then defined as 
			\begin{equation}
			\hat{Heritability} = \boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}
			\label{eq:fullShrek}
			\end{equation}
		\subsection{Calculating the \glsentrylong{se}}
			From \cref{eq:fullShrek}, we can derive the variance of heritability $H$ as 
			\begin{align}
				\mathrm{Var}(H) &= \mathrm{E}[H^2]-\mathrm{E}[H]^2\nonumber\\
				&=\mathrm{E}[(\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f})^2]-\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}](\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}])^t \nonumber \\
				&=\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{ff}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{1}]-\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}](\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}])^t \nonumber \\
				&=\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\mathrm{E}[\boldsymbol{ff}^t]\boldsymbol{R_{sq}}^{-1}\boldsymbol{1}-\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}](\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}])^t \nonumber \\
				&=\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\mathrm{Var}(\boldsymbol{f})\boldsymbol{R_{sq}}^{-1}\boldsymbol{1}+\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}](\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}])^t-\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}](\mathrm{E}[\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}])^t \nonumber\\
				&=\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\mathrm{Var}(\boldsymbol{f})\boldsymbol{R_{sq}}^{-1}\boldsymbol{1}
				\label{eq:varHvarf}
			\end{align}
			Therefore, to obtain the variance of $H$, we first need to calculate the variance covariance matrix of $\boldsymbol{f}$.
			
			We first consider the standardized genotype $X_i$ with standard normal mean $z_i$ and non-centrality parameter
			$\mu_i$, we have
			\begin{align*}
				\mathrm{E}[X_i]&=\mathrm{E}[z_i+\mu_i]\\
				&=\mu_i\\
				\mathrm{Var}(X_i) &=\mathrm{E}[(z_i+\mu_i)^2]+\mathrm{E}[(z_i+\mu_i)]^2\\
				&=\mathrm{E}[z_i^2+\mu_i^2+2z_i\mu_i]+\mu_i^2\\
				&=1 \\
				\mathrm{Cov}(X_i,X_j)&=\mathrm{E}[(z_i+\mu_i)(z_j+\mu_j)]-\mathrm{E}[z_i+\mu_i]\mathrm{E}[z_j+\mu_j]\\
				&=\mathrm{E}[z_iz_j+z_i\mu_j+\mu_iz_j+\mu_i\mu_j]-\mu_i\mu_j\\
				&=\mathrm{E}[z_iz_j]+\mathrm{E}[z_i\mu_j]+\mathrm{E}[z_j\mu_i]+\mathrm{E}[\mu_i\mu_j]-\mu_i\mu_j\\
				&=\mathrm{E}[z_iz_j]
			\end{align*}
			As the genotypes are standardized, therefore $\mathrm{Cov}(X_i,X_j)==\mathrm{Cor}(X_i,X_j)$, we can obtain
			$$
				\mathrm{Cov}(X_i,X_j)=\mathrm{E}[z_iz_j]=R_{ij}
			$$
			where $R_{ij}$ is the \gls{LD} between \gls{SNP}$_i$ and \gls{SNP}$_j$.
			Given these information, we can then calculate $\mathrm{Cov}(\chi_i^2,\chi_j^2)$ as:
			\begin{align*}
				\mathrm{Cov}(X_i^2,X_j^2)&=\mathrm{E}[(z_i+\mu_i)^2(z_j+\mu_j)^2]-\mathrm{E}[z_i+\mu_i]\mathrm{E}[z_j+\mu_j]\\
				&=\mathrm{E}[(z_i^2+\mu_i^2+2z_i\mu_i)(z_j^2+\mu_j^2+2z_j\mu_j)]-\mathrm{E}[z_i^2+\mu_i^2+2z_i\mu_i]\mathrm{E}[z_j^2+\mu_j^2+2z_j\mu_j]\\
				&=\mathrm{E}[(z_i^2+\mu_i^2+2z_i\mu_i)(z_j^2+\mu_j^2+2z_j\mu_j)]-(\mathrm{E}[z_i^2]+\mathrm{E}[\mu_i^2]+2\mathrm{E}[z_i\mu_i])(\mathrm{E}[z_j^2]+\mathrm{E}[\mu_j^2]+2\mathrm{E}[z_j\mu_j])\\
				&=\mathrm{E}[z_i^2(z_j^2+\mu_j^2+2z_j\mu_j)+\mu_i^2(z_j^2+\mu_j^2+2z_j\mu_j)+2z_i\mu_i(z_j^2+\mu_j^2+2z_j\mu_j)]-(1+\mu_i^2)(1+\mu_j^2)\\
				&=\mathrm{E}[z_i^2(z_j^2+\mu_j^2+2z_j\mu_j)]+\mu_i^2\mathrm{E}[z_j^2+\mu_j^2+2z_j\mu_j]+2\mu_i\mathrm{E}[z_i(z_j^2+\mu_j^2+2z_j\mu_j)]-(1+\mu_i^2)(1+\mu_j^2)\\
				&=\mathrm{E}[z_i^2z_j^2+z_i^2\mu_j^2+2z_i^2z_j\mu_j]+\mu_i^2+\mu_i^2\mu_j^2+2\mu_i\mathrm{E}[z_iz_j^2+z_i\mu_j^2+2z_iz_j\mu_j]-(1+\mu_i^2)(1+\mu_j^2)\\
				&=\mathrm{E}[z_i^2z_j^2]+\mu_j^2+\mu_i^2+\mu_i^2\mu_j^2+4\mu_i\mu_j\mathrm{E}[z_iz_j]-(1+\mu_i^2+\mu_j^2+\mu_i\mu_j)\\
				&=\mathrm{E}[z_i^2z_j^2]+4\mu_i\mu_j\mathrm{E}[z_iz_j]-1
			\end{align*}
			Remember that $\mathrm{E}[z_iz_j] = R_{ij}$, we then have
			$$
				\mathrm{Cov}(X_i^2, X_j^2)=\mathrm{E}[z_i^2z_j^2]+4\mu_i\mu_jR_{ij}-1
			$$
			By definition, 
			$$
				z_i|z_j\sim N(\mu_i+R_{ij}(z_j-\mu_j),1-R_{ij}^2)
			$$
			We can then calculate $\mathrm{E}[z_i^2z_j^2]$ as
			\begin{align*}
				\mathrm{E}[z_i^2z_j^2]&=\mathrm{Var}[z_iz_j]+\mathrm{E}[z_iz_j]^2\\
				&=\mathrm{E}[\mathrm{Var}(z_iz_j|z_i)]+\mathrm{Var}[\mathrm{E}[z_iz_j|z_i]]+R_{ij}^2\\
				&=\mathrm{E}[z_j^2\mathrm{Var}(z_i|z_j)]+\mathrm{Var}[z_j\mathrm{E}[z_i|z_j]]+R_{ij}^2\\
				&=(1-R_{ij}^2)\mathrm{E}[z_j^2]+\mathrm{Var}(z_j(\mu_i+R_{ij}(z_j-\mu_j)))+R_{ij}^2\\
				&=(1-R_{ij}^2)+\mathrm{Var}(z_j\mu_i+R_{ij}z_j^2-\mu_jz_jR_{ij})+R_{ij}^2\\
				&=1+\mu_i^2\mathrm{Var}(z_j)+R_{ij}^2\mathrm{Var}(z_j^2)-\mu_j^2R_{ij}^2\mathrm{Var}(z_j)\\
				&=1+2R_{ij}^2
			\end{align*}
			As a result, the variance covariance matrix of the $\chi^2$ variances represented as
			\begin{equation}
				\mathrm{Cov}(X_i^2,X_j^2) = 2R_{ij}^2+4R_{ij}\mu_i\mu_j
				\label{eq:finalChi}
			\end{equation}
			Considering that we only have the \emph{observed} expectation, we should re-define \cref{eq:finalChi} as
			\begin{equation}
				\mathrm{Cov}(X_i^2,X_j^2) = \frac{2R_{ij}^2+4R_{ij}\mu_i\mu_j}{n^2}
				\label{eq:finalChiCov}
			\end{equation}
			where $n$ is the sample size.
			
			By substituting \cref{eq:finalChiCov} into \cref{eq:varHvarf}, we will get
			\begin{align}
				\mathrm{Var}(H) &=\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\frac{2\boldsymbol{R_{sq}}+4\boldsymbol{R}\circ \boldsymbol{zz}^t}{n^2}\boldsymbol{R_{sq}}^{-1}\boldsymbol{1}
				\label{eq:covH}
			\end{align}
			where $\boldsymbol{z} = \sqrt{\boldsymbol{\chi^2}}$ from \cref{eq:defineF}, with the direction of effect as its sign and $\circ$ is the element-wise product (Hadamard product).
			 
			Problem with \cref{eq:covH} were that not only does it requires the direction of effect, the error in the \gls{LD} matrix also tends to amplify due to its predominant role in the equation, leading to un-stable estimation of the \gls{se}.
			 
			Another way to get the \gls{se} is based on the fact that $\boldsymbol{f}$ is approximately $\chi^2$ distributed. 
			Therefore \cref{eq:shrekEq} can be viewed as a decomposition of a vector of $\chi^2$ distributions with degree of freedom of 1. 
			Replacing the vector $\boldsymbol{f}$ with a vector of 1, we can perform the decomposition of the degree of freedom, getting the ``effective number''($e$) of the association\citep{Li2011}. 
			%The problem of this effective number is that they uses the eigenvalue instead of this multiplication.
			%So either we have to explain why we don't follow it (therefore explaining the slidding windows) or we should just avoid mentioning the effective number
			Substituting $e$ into the variance equation of non-central $\chi^2$ distribution will yield
			\begin{equation}
			\mathrm{Var}(H) = \frac{2(e+2H)}{n^2}
			\label{eq:effectiveChi}
			\end{equation}
			 \cref{eq:effectiveChi} will gives us an heuristic estimation of the \gls{se}. 
		\subsection{Case Control Studies}	 
		%Discuss on the liability threshold model. Then the apply orange paper. Then explain how to get the results. 
			When dealing with case control data, as the phenotype were usually discontinuous, we cannot directly use \cref{eq:fullShrek} to estimate the heritability.
			Instead, we will need to employ the concept of liability threshold model from \cref{sec:liability}. 
			
			Based on the derivation of \cite{Yang2010}, the approximate ratio between the \gls{ncp} obtained from case control studies ($NPC_{CC}$) and quantitative trait studies($NCP_{QT}$) were
		
			\begin{equation}
			\frac{NCP_{CC}}{NCP_{QT}} = \frac{i^2v(1-v)N_{CC}}{(1-K)^2N_{QT}}
			\label{eq:originNCPTransform}
			\end{equation}
			where
			\begin{align*}
			 K &= \text{Population Prevalence} \\
			 v &= \text{Proportion of Cases}\\
			 N &= \text{Total Number of Samples}\\
			 i &= \frac{z}{K}\\
			 z &= \text{height of standard normal curve at truncation pretained to K}
			\end{align*}
			
			Based on this observation, we can then directly transform the \gls{ncp} between the case control studies and quantitative trait studies.
			During the transformation, $N_{CC}$ and $N_{QT}$ will be the same, therefore \cref{eq:originNCPTransform} became
			\begin{equation}
			NCP_{QT} = \frac{NCP_{CC}(1-K)^2}{i^2v(1-v)}
			\label{eq:transform}
			\end{equation}
			
			By combining \cref{eq:transform} and \cref{eq:defineF}, we can then have
			\begin{equation}
			f = \frac{(\chi^2_{CC}-1)(1-K)^2}{ni^2v(1-v)}
			\end{equation}
			where $\chi^2_{CC}$ is the test statistic from the case control association test.
			Finally, the heritability estimation of case control studies can be simplified to 
			\begin{equation}
			\hat{Heritability} =\frac{(1-K)^2}{i^2v(1-v)} \boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}
			\label{eq:caseControlHerit}
			\end{equation}
			
		\subsection{Extreme Phenotype Selections}
			%Explain why we perform extreme phenotype selections. Explain how that affect the variance of the estimation. Finally, explain how to perform heritability estimation on extreme phenotype. 
			When extreme phenotype selection were performed, the variance of the selected phenotype will not be representative of that in the population.
			Most notably, the variance of the post selection phenotype will tends to increase.
			Thus, to adjust for this bias, one can multiple the estimated heritability $\hat{h^2}$ by the ratio between the variance before $V_P$ and after $V_{P'}$ the selection process\citep{Sham2014}:
			
			\begin{equation}
			\hat{Heritability} = \frac{V_{P'}}{V_P}\boldsymbol{1}^t\boldsymbol{R_{sq}}^{-1}\boldsymbol{f}
			\label{eq:extremeShrek}
			\end{equation}
			
		\subsection{Calculating the \glsentrylong{LD} matrix}
			% Might want to remove this section as we no longer use this correction
			To estimate the heritability, the population \gls{LD} matrix is required.
			In reality, one can only obtain the \gls{LD} matrix based on a subset of the population (e.g. the 1000 genome project\citep{Project2012} or the HapMap project\citep{Altshuler2010}).
			There are therefore sampling errors among the \gls{LD} elements. 
			
			Now if we consider \cref{eq:fullShrek}, the $\boldsymbol{R_{sq}}$ matrix is required.
			As the squared \gls{LD} is used, a positive bias is induced into our $\boldsymbol{R_{sq}}$ matrix. 
			
			Based on \citet{Shieh2010}, one can correct for bias in the Pearson correlation $\rho$ using
			\begin{equation}
			\rho = \rho\{1+\frac{1-\rho^2}{2(N-4)}\}
			\label{eq:rhoCorrect}
			\end{equation}
			where $N$ is the number of sample used in the calculation of $\rho$. 
			Similarly, there exists a bias correction equation for $\rho^2$:
			\begin{equation}
				\rho^2=1-\frac{N-3}{N-2}(1-\rho^2)\{1+\frac{2(1-\rho^2)}{N-3.3}\}
				\label{eq:rho2Correct}
			\end{equation}
			Therefore, we corrected the $\boldsymbol{R_{sq}}$ based on \cref{eq:rho2Correct} such that the bias in estimation can be minimized. 
		\subsection{Inverse of the \glsentrylong {LD} matrix}
			In order to obtain the heritability estimation, we will require to solve \cref{eq:fullShrek}. 
			If $\boldsymbol{R_{sq}}$ is of full rank and positive semi-definite, it will be straight-forward to solve the matrix equation.
			However, more often than not, the \gls{LD} matrix are rank-deficient and suffer from multicollinearity, making it ill-conditioned, therefore highly sensitive to changes or errors in the input.
			To be exact, we can view \cref{eq:fullShrek} as calculating the sum of $\boldsymbol{\hat{h^2}}$ from  \cref{eq:shrekEq}.
			This will involve solving for
			\begin{equation}
			\boldsymbol{\hat{h^2}} = \boldsymbol{R_{sq}}^{-1}\boldsymbol{f}
			\label{eq:shrekInverse}
			\end{equation}
			where an inverse of $\boldsymbol{R_{sq}}$ is observed. 
			
			In normal circumstances (e.g. when $\boldsymbol{R_{sq}}$ is full rank and positive semi-definite), one can easily solve \cref{eq:shrekInverse} using the QR decomposition or LU decomposition.
			However, when $\boldsymbol{R_{sq}}$ is ill-conditioned, the traditional decomposition method will fail.
			Even if the decomposition is successfully performed, the result tends to be a meaningless approximation to the true $\boldsymbol{\hat{h^2}}$. 
			
			Therefore, to obtain a meaningful solution, regularization techniques such as the Tikhonov Regularization (also known as Ridge Regression) and \gls{tSVD} has to be performed\citep{Neumaier1998}. 
			There are a large variety of regularization techniques, yet the discussion of which is beyond the scope of this study. 
			In this study, we will focus on the use of \gls{tSVD} in the regularization of the \gls{LD} matrix.
			This is because the \gls{SVD} routine has been implemented in the EIGEN C++ library \citep{eigenweb}, allowing us to implement the \gls{tSVD} method without much concern with regard to the detail of the algorithm. 
			
			To understand the problem of the ill-conditioned matrix and regularization method, we consider the matrix equation $\boldsymbol{Ax}=\boldsymbol{B}$ where $\boldsymbol{A}$ is ill-conditioned or singular with $n\times n$ dimension.
			The \gls{SVD} of $\boldsymbol{A}$ can be expressed as 
			\begin{align}
				\boldsymbol{A} = \boldsymbol{U\Sigma V}^t
				\label{eq:svd}
			\end{align}
			where $\boldsymbol{U}$ and $\boldsymbol{V}$ are both orthogonal matrix and $\boldsymbol{\Sigma}=\mathrm{diag}(\sigma_1,\sigma_2,\dots,\sigma_n)$ is the diagonal matrix of the \emph{singular values}($\sigma_i$) of matrix $\boldsymbol{A}$.
			Based on \cref{eq:svd}, we can get the inverse of $\boldsymbol{A}$ as 
			\begin{align}
				\boldsymbol{A}^{-1}= \boldsymbol{V\Sigma}^{-1}\boldsymbol{U}^t
				\label{eq:svdInverse}
			\end{align}
			Where $
			\boldsymbol{\Sigma}^{-1} = \mathrm{diag}(\frac{1}{\sigma_1},\frac{1}{\sigma_2},\dots,\frac{1}{\sigma_n})$.
			Now if we consider there to be error within $\boldsymbol{B}$ such that
			\begin{equation}
				\boldsymbol{\hat{B_i}} = \boldsymbol{B_i}+\epsilon_i
				\label{eq:errorB}
			\end{equation}
			we can then represent $\boldsymbol{Ax}=\boldsymbol{B}$ as
			\begin{align}
				\boldsymbol{Ax}&=\boldsymbol{\hat{B}} \nonumber\\
				\boldsymbol{U\Sigma V}^t\boldsymbol{x}&=\boldsymbol{\hat{B}} \nonumber\\
				\boldsymbol{x}&=\boldsymbol{V\Sigma}^{-1}\boldsymbol{U}^t\boldsymbol{\hat{B}}
				\label{eq:solveBwithError}
			\end{align}
			A matrix $\boldsymbol{A}$ is considered as ill-condition when its condition number $\kappa(\boldsymbol{A})$ is large or singular when its condition number is infinite. 
			One can represent the condition number as $\kappa(\boldsymbol{A})=\frac{\sigma_1}{\sigma_n}$.
			Therefore it can be observed that when $\sigma_n$ is tiny, $\boldsymbol{A}$ is likely to be ill-conditioned and when $\sigma_n=0$, $\boldsymbol{A}$ will be singular. 
			
			One can also observe from \cref{eq:solveBwithError} that when the singular value $\sigma_i$ is small, the error $\epsilon_i$ in \cref{eq:errorB} will be drastically magnified by a factor of $\frac{1}{\sigma_i}$. 
			Making the system of equation highly sensitive to errors in the input.
			
			To obtain a meaningful solution from this ill-conditioned/singular matrix $\boldsymbol{A}$, we may perform the \gls{tSVD} method to obtain a pseudo inverse of $\boldsymbol{A}$.
			Similar to \cref{eq:svd}, the \gls{tSVD} of $\boldsymbol{A}$ can be represented as 
			\begin{alignat}{2}
				&\boldsymbol{A}^+ = \boldsymbol{U\Sigma}_k\boldsymbol{V}^t  &\qquad\text{and}\qquad  &\boldsymbol{\Sigma}_k=\mathrm{diag}(\sigma_1,\dots,\sigma_k,0,\dots,0)
				\label{eq:tsvd}				
			\end{alignat}
			where $\boldsymbol{\Sigma}_k$ equals to replacing the smallest $n-k$ singular value replaced by 0 \citep{Hansen1987}. 
			Alternatively, we can define
			\begin{equation}
			\sigma_i=\begin{cases}
			\sigma_i\qquad\text{for}\qquad\sigma_i\ge t\\
			0\qquad\text{for}\qquad\sigma_i<t
			\end{cases}
			\end{equation}
			where $t$ is the tolerance threshold. 
			Any singular value $\sigma_i$ less than the threshold will be replaced by 0. 
			
			By selecting an appropriate $t$, \gls{tSVD} can effectively regularize the ill-conditioned matrix and help to find a reasonable approximation to $x$. 
			A problem with \gls{tSVD} however is that it only work when matrix $\boldsymbol{A}$ has a well determined numeric rank\citep{Hansen1987}.
			That is, \gls{tSVD} work best when there is a large gap between $\sigma_k$ and $\sigma_{k+1}$.
			If a matrix has ill-conditioned rank, then $\sigma_k-\sigma_{k+1}$ will be small.
			For any threshold $t$, a small error can change whether if $\sigma_{k+1}$ and subsequent singular values should be truncated, leading to unstable results. 
			
			According to \citet{Hansen1987}, matrix where its rank has meaning will have well defined rank. 
			As \gls{LD} matrix is the correlation matrix between each individual \glspl{SNP}, the rank of the \gls{LD} matrix is the maximum number of linear independent \glspl{SNP} in the region, therefore likely to have a well-defined rank. 
			The easiest way to test whether if the threshold $t$ and whether if the matrix $\boldsymbol{A}$ has well-defined rank is to calculate the ``gap'' in the singular value:
			\begin{equation}
			gap = \sigma_k/\sigma_{k+1}
			\label{eq:gapSingular}
			\end{equation}
			a large gap usually indicate a well-defined gap. 
			
			In this study, we adopt the threshold as defined in MATLAB, NumPy and GNU Octave: $t=\epsilon\times\mathrm{max}(m,n)\times\mathrm{max}(\boldsymbol{\Sigma})$ where $\epsilon$ is the machine epsilon (the smallest number a machine can define as non-zero). 
			And we perfomed a simulation study to investigate the performance of \gls{tSVD} under the selected threshold.
			Ideally, if the ``gap'' is large under the selected threshold, then \gls{tSVD} will provide a good regularization to the equation. 
			
			1,000 samples were randomly simulated from the HapMap\citep{Altshuler2010} \acrshort{CEU} population with
			1,000 \glspl{SNP} randomly select from chromosome 22. 
			The \gls{LD} matrix and its corresponding singular value were calculated. 
			The whole process were repeated 50 times and the cumulative distribution of the ``gap'' of singular values were plotted (\cref{fig:singularValueDist}). 
			It is clearly show that the \gls{LD} matrix has a well-defined rank with a mean of maximum ``gap'' of 466,198,939,298.
			Therefore the choice of \gls{tSVD} for the regularization is appropriate.
			%\begin{wrapfigure}{L}{3in}
			\begin{figure}
				\caption[Cumulative Distribution of ``gap'' of the LD matrix]{Cumulative Distribution of ``gap'' of the LD matrix, the vertical line indicate the full rank. It can be observed that there is a huge increase in ``gap'' before full rank is achieved. Suggesting that the rank of the LD matrix is well defined}
				\centering
				\includegraphics[width=0.5\textwidth]{figure/singular_value_distribution.png}
				\label{fig:singularValueDist}
				\vspace{-20pt}
			\end{figure}
			%\end{wrapfigure}
			
			By employing the \gls{tSVD} as a method for regularization, we were able to solve the ill-posed \cref{eq:shrekEq}, and obtain the estimated heritability.
						
		\subsection{Comparing with \glsentrylong{ldsc}}
			% main difference 
			Conceptually, the fundamental hypothesis of \gls{ldsc} and \gls{shrek} were quite different.
			\gls{ldsc} were based on the ``global'' inflation of test statistic and its relationship to the \gls{LD} pattern and by using this relationship, \gls{ldsc} might estimate the heritability.
			On the other hand, \gls{shrek} focuses more on the per-\gls{SNP} level.
			Our main idea with \gls{shrek} was that the test statistic of each \glspl{SNP} should be representative to the amount of variation they explain when there is no \gls{LD}. 
			Thus, based on this concept, \gls{shrek} targeted in ``removing'' the inflation of test statistic introduced through the \gls{LD} between \glspl{SNP} and the heritability can be calculated by adding the test statistic of all \glspl{SNP} after ``removing'' the inflation. 
			
			Mathematically, the calculation of \gls{ldsc} and \gls{shrek} were very different. 
			\gls{ldsc} take the sum of all $R^2$ within a 1cM region as the LD score and regress it against the test statistic to obtain the slope and intercept which represent the heritability and amount of confounding factors respectively. 
			In their model, \gls{ldsc} assume that each \glspl{SNP} will explain the same portion of heritability
			\begin{align}
			 \mathrm{Var}(\beta)&=\frac{h^2}{M}\boldsymbol{I}\\
			 M &= \text{number of SNPs}\notag\\
			 \beta &= \text{vector containing per normalized genotype effect sizes}\notag\\
			 I &= \text{identity matrix}\notag\\
			 h^2 &= \text{heritability}\notag
			\end{align}
			
			As for \gls{shrek}, the whole \gls{LD} matrix were used and inverted to decompose the \gls{LD} from the test statistic. 
			There were no assumption of the amount of heritability explained by each \glspl{SNP}. 
			However, \gls{shrek} does assumed that the null should be 1 and therefore \gls{shrek} cannot detect the amount of confounding factors. 
					
	\section{Simulation}
		We implemented the heritability estimation in \gls{shrek}.
		To assess how well \gls{shrek} performs for heritability estimation in comparison to other current methods, we performed series of systematic simulation.
		In these simulations, performance of \gls{shrek}, \gls{gcta}\citep{Yang2011} and \gls{ldsc}\citep{Bulik-Sullivan2015} with and without the intercept estimation function (-{}-no-intercept) were tested.
		Through simulation, we can obtain the sample distribution of the heritability estimate under different study designs (e.g. Quantitativat traits, Case-Control studies or extreme phenotype selection). 
		%We can also evaluate the performance of different methods under varying genetic architecture (e.g. different number of Snps, different LD structures) or even with different disease models (e.g. different number of causal Snps, different heritability).
		Factors considered in our simulations were as follow:		\subsection{Sample Size}
		The sample size was one of the most important parameter in determining the standard error of the heritability estimation. 
		As the sample size increases, study will be more representative of the true population and therefore generate a more stable and accurate results.
		
		Using simple text mining technique, we have calculated the average sample size for all \gls{GWAS} recorded on the \gls{GWAS} catalog\citep{Welter2014} was around 7,200 samples.
		We argued that if the tools were able to preform well when the sample size was small, then they should have no problem handling larger data sets for the sample size only affect the standard error and should not bias the estimate.
		Thus, we only simulate 1,000 samples in our simulation to test the performance of the tools when a small sample size was provided.
		
		\subsection{Number of SNPs in Simulation}
		As technology progress, the number of \glspl{SNP} on each array chip has increased significantly.
		We can now achieve a single base pair resolution using the \gls{ngs} technologies.
		However, in simulation, the number of \glspl{SNP} is one of the biggest determinant of speed. 
		The time required for simulation raised form 2 hours to 2 days when the number of \glspl{SNP} increased from 3,000 to 50,000. 
		Considering that one usually partition the whole genome into individual chromosomes during the heritability estimation, and that it was infeasible for us to simulate \glspl{SNP} number similar to those encountered in real genotype chip, we compromise the number of \glspl{SNP} in our simulation to 50,000 \glspl{SNP} from chromosome 1.
		
		\subsection{Genetic Architecture} % Should contain the effect size distribution, the causal SNP number and the heritability spectrum
		Of all the simulation parameter, the genetic architecture was arguably the most complicated parameter. 
		It involves the \gls{LD} pattern, the distribution of effect size, the number of causal \glspl{SNP}, the \gls{maf} of the causal \glspl{SNP} and most importantly, the heritability of the trait ($h^2$).
		
		Because we would like to cover most of the heritability spectrum, we will simulate traits with $h^2$ ranging from 0 to 0.9, with increment of 0.1 such that $h^2 \in \{0,\allowbreak 0.1,\allowbreak 0.2,\allowbreak 0.3,\allowbreak 0.4,\allowbreak 0.5,\allowbreak 0.6,\allowbreak 0.7,\allowbreak 0.8,\allowbreak 0.9\}$.
		To simplify the condition, all ``causal'' variants were included in the simulation (e.g. perfect tagging).
		We also try to obtain realistic \gls{LD} pattern by using HAPGEN2\citep{Su2011} to simulate genotype based on the \gls{LD} pattern of the 1000 genome \gls{CEU} samples. 
		
		First, we only consider situation of common \glspl{SNP} and only simulate \glspl{SNP} with \gls{maf} $> 0.1$.
		In these simulation, we varies the number of causal \glspl{SNP} ($k$) and the effect size distribution.
		We consider $k\in\{5, 10, 50, 100, 250, 500\}$ such that we can cover different disease spectrum (Oligogenic to Polygenic diseases). 
		As for effect size distribution, we considered two conditions:
		\begin{enumerate}
			\item Equal Effect Size
			\item Random Effect Size
		\end{enumerate}
		The simplest situation was when all casual \glspl{SNP} have the same effect size 
		\begin{equation}
		\beta_s=\pm\sqrt{\frac{h^2}{k}}
		\label{eq:stableEffect}
		\end{equation}
		The direction of effect should be randomly simulated.
		As for the random effect size scenario, we consider the effect size to be 
		\begin{equation}
		\beta_r=\pm\sqrt{\frac{\gamma \times h^2}{\sum \gamma}}
		\label{eq:randomEffect}
		\end{equation}
		with $\gamma\sim exp(\lambda=1)$ and a random direction of effect.
		Rationale behind the choice of exponential distribution was based on \citet{Orr1998}, which suggested that exponential distribution with $\lambda=1$ may serve as a heuristic expectation the genetic architecture of adaptation.

		Once the effect size was calculated, we can then randomly assign the effect size to $k$ random \glspl{SNP} with normalized genotype $\boldsymbol{X}$, the phenotype can then be calculated as 
		\begin{align}
		\epsilon_i&\sim N(0,\sqrt{\mathrm{Var}(\boldsymbol{X\beta})\frac{1-h^2}{h^2}} )\notag\\
		\boldsymbol{\epsilon} &= (\epsilon_1,\epsilon_2,...,\epsilon_n)^t\notag\\
		\boldsymbol{y} &= \boldsymbol{X\beta}+\boldsymbol{\epsilon}
		\label{eq:simulationOfPhenotype}
		\end{align}
		
		For each batch of simulated samples, we calculate the estimated heritability using \gls{shrek}, \gls{gcta}, \gls{ldsc} with intercept fixed at 1 and \gls{ldsc} allowing for intercept estimation for each $h^2$.
		In each iteration, the sample genotype was provided to \gls{gcta} for the calculation of genetic relationship matrix (GRM) whereas for \gls{shrek} and \gls{ldsc}, 500 independent samples were simulated based on the 1000 genome project \gls{CEU} samples\parencite{Project2012} to construct the \gls{LD} matrix and calculate the \gls{LD} score respectively.
		This was because both \gls{ldsc} and \gls{shrek} were designed to work in situation where the raw genotype were not provided and the \gls{LD} structure was usually obtained form the public data base instead.
		Therefore to provide a realistic simulation, an independent set of reference samples were provided for \gls{shrek} and \gls{ldsc}.
		
		The whole process were repeated 50 times with the same \glspl{SNP} set, the same causal \glspl{SNP} and the same effect size for each $h^2$ such than an empirical variance can be obtained.
		We then repeat the process 10 times on different \glspl{SNP} sets, with different causal \glspl{SNP} but the same effect size for each $h^2$.
		This should introduce a slight variation in the \gls{LD} structure and should demonstrate the robustness of the programmes under different \gls{LD} construct.
		To summarize, the simulation procedure follows:
		\begin{enumerate}
			\item Randomly select 50,000 \glspl{SNP} with \gls{maf}$>0.1$ from chromosome 1
			\item Randomly generate $k$ effect size with $k \in \{5,10,50,100,250,500\}$ following either \cref{eq:stableEffect} or \cref{eq:randomEffect}
			\item Randomly assign the effect size to $k$ \glspl{SNP}
			\item Simulate 1,000 samples using HAPGEN2 and calculate their phenotype according to \cref{eq:simulationOfPhenotype}
			\item Perform heritability estimation using \gls{shrek}, \gls{ldsc} and \gls{gcta}
			\item Repeat step 4-5 50 times
			\item Repeat step 1-6 10 times
		\end{enumerate}
		
		\subsubsection{Extreme Effect Size}
		Another condition we were interested in was the performance of the tools when there is a small amount of \glspl{SNP} that explain a large portion of effect e.g. 50\%.
		Similarly, we only consider 1,000 samples, with 50,000 common \glspl{SNP}(\gls{maf} $>0.1$).
		We hypothesize that under the polygenic model, such extreme distribution in effect size should have much larger effect when compared to that in oligogenic condition. 
		Thus we only consider the polygenic condition where the number of causal \gls{SNP} ($k$) were limited to 100 or 250. 
		
		We simulate $m$ \glspl{SNP} accounting for 50\% of all the effect where $m\in\{1,5,10\}$.
		The effect size was then calculated as
		\begin{align}
		\beta_{eL} &= \pm\sqrt{\frac{0.5h^2}{m}} 
		\beta_{eS} &= \pm\sqrt{\frac{0.5h^2}{k-m}} 
		\beta &= \{\beta_{eL}, \beta_{eS}\}
		\label{eq:extremEffect}
		\end{align}
		the effect size were then randomly assigned to $k$ causal \glspl{SNP} and phenotype was calculated as in \cref{eq:simulationOfPhenotype}.
		The simulation procedure then becomes
		\begin{enumerate}
			\item Randomly select 50,000 \glspl{SNP} with \gls{maf}$>0.1$ from chromosome 1
			\item Randomly generate $k$ effect size with $k \in \{100,250\}$ and $m$ extreme effect, following \cref{eq:extremEffect} where $m\in{1,5,10}$
			\item Randomly assign the effect size to $k$ \glspl{SNP}
			\item Simulate 1,000 samples using HAPGEN2 and calculate their phenotype according to \cref{eq:simulationOfPhenotype}
			\item Perform heritability estimation using \gls{shrek}, \gls{ldsc} and \gls{gcta}
			\item Repeat step 4-5 50 times
			\item Repeat step 1-6 10 times
		\end{enumerate}
		\subsubsection{Rare SNPs}
		Based on the ``common disease - rare variant'' hypothesis, a complex disease might still be affected by number of rare genetic variants. 
		With the advancement of technology, there are now genotype arrays such as exome chip which aim to detect some of the rare variants. 
		Thus we were interested in testing the performance of the tools under such extreme scenario.
		
		Here, we will consider the situation where of the $k$ causal \glspl{SNP}, $r$ of them were rare (e.g. \gls{maf}$<0.05$).
		Instead of simulating only common \glspl{SNP}, we simulate 50,000 \glspl{SNP} with \gls{maf} $>0.01$. 
		Then we simulate $k$ effect size using equation \cref{eq:stableEffect}.
		However, instead of randomly distributing the effect size to $k$ random \glspl{SNP}, we distribute the effect size to $r$ rare \glspl{SNP} (\gls{maf} $<0.05$) and $k-r$ common \glspl{SNP} (\gls{maf}$>0.05$) with $r\in\{5,10\}$.
		Again, the phenotype was calculated using \cref{eq:simulationOfPhenotype}.
		
		\subsection{Case Control Studies}
		The simulation of case control studies was very much like that of the simulation of quantitative trait. 
		However, there were two additional parameters to consider: the population prevalence and the observed prevalence.
		These parameters allow us to simulate the samples under a liability model, therefore simulating the case control studies.

		Although there were only two additional parameter, the computational challenge for the simulation of case control was significantly bigger than that for the simulation of quantitative trait.
		Take for example, if one like to simulate a trait with population prevalence of $p$ and observed prevalence  of $q$ and would like to have $n$ cases in total, one will have to simulate $\min(\frac{n}{p}, \frac{n}{q})$ samples.
		Considering the scenario where the observed prevalence is 50\%, the population prevalence is 1\% and 1,000 cases,a minimum of 100,000 samples will be required.
		If each samples have 50,000 \glspl{SNP}, then one will need to process a minimum of 40GB of data per iteration.
		
		Given limited computer resources, we simulate 1,000 cases with $q=0.5$ and varies $p\in\{0.5,0.1\}$ as a prove of concept simulation. 
		We argue that although we only consider a limited combination of prevalence, the result should be generalize to other combination of prevalence.
		
		
		
		\subsection{Extreme Phenotype Selection}
		\subsection{Case Control Studies}
		Similar to quantitative trait simulation, the sample size, the number of \glspl{SNP}, the number of causal \glspl{SNP} and the true heritability of the trait are important parameters to consider during simulation. 
		On top of that, there are a few more parameters one must consider during the simulation of case control studies such as the population prevalence of the trait and the observed prevalence of the study. 
		
		To simulate cases and controls, we will need to simulate the liability distribution by taking into acount of the prevalence. 
		So for example, if one like to simulate a trait with population prevalence of $p$ and observed prevalence  of $q$ and would like to have $n$ cases in total, one will have to simulate $\min(\frac{n+pn}{p}, \frac{n+qn}{q})$ samples.
		It is therefore challenging for one to simulate scenario with a small $p$ or $q$ values. 
		
		In this study, we fixed $q=0.5$ and varies $p\in\{0.5,0.1\}$. Although disease such as \gls{scz} can have a prevalence $\approx1\%$, the required sample numbers become infeasible for large scale simulation where a minimum of 101,000 samples will be required if we wish to obtain 1,000 cases.
		Despite our wish to simulate conditions with small prevalence, the limitation of computation power simply forbade us to undergo such simulation. 
		
		Once the liability distribution were simulated, cases can be drawn from samples with a liability higher than the liability threshold. 
		The liability threshold was calculated as the value $> 1-p$ of all values under the standard normal distribution using the \emph{qnorm} function in R. 
		Samples with a liability lower than the liability threshold were then considered as control samples. 
		1,000 cases and 1,000 controls were then randomly drawn from the corresponding population of samples.
		 
		To summarize, 
		\begin{enumerate}
			\item Randomly select 50,000 \glspl{SNP} from chromosome 1
			\item Randomly generate $k$ effect size following \cref{eq:snpEffect} where $k \in \{10,50,100,1000\}$
			\item Randomly assign the effect size to $k$ \glspl{SNP} where \glspl{SNP} with small \gls{maf} will get a large effect size.
			\item Simulate $\frac{n+qn}{q}$ samples using HAPGEN2 where $q\in\{0.5,0.1\}$
			\item Simulate sample phenotype according to the liability threshold where 1,000 cases and 1,000 controls were obtained
			\item Perform heritability estimation using \gls{shrek}, \gls{ldsc} and  \gls{gcta}
			\item Repeat step 4-6 50 times
			\item Repeat step 1-7 50 times
		\end{enumerate}
		
		\subsection{Exreme Phenotype Selections}
		The simulation of extreme phenotype selection is very much like the combination of quantitative trait simulation. 
		The simplest way to simulate the extreme phenotype selection is to first simulation $N$ samples, then from this population of samples, select $n$ samples from both end of the population and use them to perform association and heritability estimation.
		
		Due to the similarity in nature with the simulation of quantitative traits and case control studies, we limit the number of causal \glspl{SNP} simulated as 100. 
		We also limit our simulation to select samples with phenotype on the top and bottom $10\%$ of the population.
		
		However, it was noted that both \gls{gcta} and \gls{ldsc} did not implement heritability estimation under extreme phenotype selection. 
		To perform a fair comparison with \gls{shrek}, we calculate the adjustment factor $\frac{\mathrm{Var}(Phenotype\ before\ selection)}{\mathrm{Var}(Phenotype\ after\ selection)}$ to the estimation from \gls{gcta} and \gls{ldsc}.
		
		Therefore, to summarize,
		\begin{enumerate}
			\item Randomly select 50,000 \glspl{SNP} from chromosome 1
			\item Randomly generate 100 effect size following \cref{eq:snpEffect}
			\item Randomly assign the effect size to $k$ \glspl{SNP} where \glspl{SNP} with small \gls{maf} will get a large effect size.
			\item Simulate 10,000 samples using HAPGEN2 and calculate their phenotype according to \cref{eq:simulationOfPhenotype}
			\item Select the top and bottom $10\%$ of samples from the 10,000 samples.
			\item Perform heritability estimation using \gls{shrek}, \gls{ldsc} and  \gls{gcta}
			\item Manually apply the adjustment factor to estimation of \gls{gcta} and \gls{ldsc}
			\item Repeat step 4-7 50 times
			\item Repeat step 1-8 50 times
		\end{enumerate}
		
	\section{Result}
		The heritabilibty estimation were implemented in \gls{shrek} and is available on \url{https://github.com/choishingwan/shrek}.  
		% We need to find a way to explain our results in a logical sense
		
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/shrek_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/gcta_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/ldsc_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/ldscIn_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanI}
			}
			\caption[Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 10 causal variants of same effect size]
			{Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 10 causal variants with same effect size.
				It was observed that of all the tools, \gls{shrek} performed best in such scenario} 
			\label{fig:50k10cQtMean}
		\end{figure}
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/shrek_50k_10c_varH.png}}
				\label{fig:50k10cQtvarS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/gcta_50k_10c_varH.png}}
				\label{fig:50k10cQtvarG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/ldsc_50k_10c_varH.png}}
				\label{fig:50k10cQtvarL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/10c/ldscIn_50k_10c_varH.png}}
				\label{fig:50k10cQtvarI}
			}
			\label{fig:50k10cQtVar}
		\end{figure}
		
		
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/shrek_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/gcta_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/ldsc_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/ldscIn_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanI}
			}
			\caption[Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 50 causal variants of same effect size]
			{Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 50 causal variants with same effect size.} 
			\label{fig:50k50cQtMean}
		\end{figure}
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/shrek_50k_50c_varH.png}}
				\label{fig:50k50cQtvarS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/gcta_50k_50c_varH.png}}
				\label{fig:50k50cQtvarG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/ldsc_50k_50c_varH.png}}
				\label{fig:50k50cQtvarL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/50c/ldscIn_50k_50c_varH.png}}
				\label{fig:50k50cQtvarI}
			}
			\label{fig:50k50cQtVar}
		\end{figure}
		
		
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/shrek_50k_100c_meanH.png}}
				\label{fig:50k100cQtmeanS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/gcta_50k_100c_meanH.png}}
				\label{fig:50k100cQtmeanG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/ldsc_50k_100c_meanH.png}}
				\label{fig:50k100cQtmeanL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/ldscIn_50k_100c_meanH.png}}
				\label{fig:50k100cQtmeanI}
			}
			\caption[Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 100 causal variants of same effect size]
			{Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 100 causal variants with same effect size.} 
			\label{fig:50k100cQtMean}
		\end{figure}
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/shrek_50k_100c_varH.png}}
				\label{fig:50k100cQtvarS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/gcta_50k_100c_varH.png}}
				\label{fig:50k100cQtvarG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/ldsc_50k_100c_varH.png}}
				\label{fig:50k100cQtvarL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/100c/ldscIn_50k_100c_varH.png}}
				\label{fig:50k100cQtvarI}
			}
			\label{fig:50k100cQtVar}
		\end{figure}
		
		
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/shrek_50k_250c_meanH.png}}
				\label{fig:50k250cQtmeanS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/gcta_50k_250c_meanH.png}}
				\label{fig:50k250cQtmeanG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/ldsc_50k_250c_meanH.png}}
				\label{fig:50k250cQtmeanL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/ldscIn_50k_250c_meanH.png}}
				\label{fig:50k250cQtmeanI}
			}
			\caption[Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 250 causal variants of same effect size]
			{Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 250 causal variants with same effect size.} 
			\label{fig:50k250cQtMean}
		\end{figure}
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/shrek_50k_250c_varH.png}}
				\label{fig:50k250cQtvarS}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/gcta_50k_250c_varH.png}}
				\label{fig:50k250cQtvarG}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/ldsc_50k_250c_varH.png}}
				\label{fig:50k250cQtvarL}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/same_effect/250c/ldscIn_50k_250c_varH.png}}
				\label{fig:50k250cQtvarI}
			}
			\label{fig:50k250cQtVar}
		\end{figure}
		
		
		
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/shrek_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanSre}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/gcta_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanGre}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/ldsc_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanLre}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/ldscIn_50k_10c_meanH.png}}
				\label{fig:50k10cQtmeanIre}
			}
			\caption[Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 10 causal variants of random effect size]
			{Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 10 causal variants with random effect size.} 
			\label{fig:50k10cQtMeanre}
		\end{figure}
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/shrek_50k_10c_varH.png}}
				\label{fig:50k10cQtvarSre}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/gcta_50k_10c_varH.png}}
				\label{fig:50k10cQtvarGre}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/ldsc_50k_10c_varH.png}}
				\label{fig:50k10cQtvarLre}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/10c/ldscIn_50k_10c_varH.png}}
				\label{fig:50k10cQtvarIre}
			}
			\label{fig:50k10cQtVarre}
		\end{figure}
		
		
		
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/shrek_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanSre}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/gcta_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanGre}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/ldsc_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanLre}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/ldscIn_50k_50c_meanH.png}}
				\label{fig:50k50cQtmeanIre}
			}
			\caption[Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 50 causal variants of random effect size]
			{Simulation of Quantitative Traits with 50k \glsentryshortpl{SNP} and 50 causal variants with random effect size.} 
			\label{fig:50k50cQtMeanre}
		\end{figure}
		\begin{figure}
			\centering
			\centering
			\subfloat[SHREK]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/shrek_50k_50c_varH.png}}
				\label{fig:50k50cQtvarSre}
			}
			\subfloat[GCTA]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/gcta_50k_50c_varH.png}}
				\label{fig:50k50cQtvarGre}
			}\\
			\subfloat[LDSC with fix intercept]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/ldsc_50k_50c_varH.png}}
				\label{fig:50k50cQtvarLre}
			}
			\subfloat[LDSC with intercept estimation]{
				\scalebox{.4}{\includegraphics{figure/quantitative/random_effect/50c/ldscIn_50k_50c_varH.png}}
				\label{fig:50k50cQtvarIre}
			}
			\label{fig:50k50cQtVarre}
		\end{figure}
		
		
	\section{Discussion}